---
title: "class 8: Breast Cancer Mini Project"
author: "Aigerim (PID: 09919142)"
format: pdf
---

The goal of this mini-project is for you to explore a complete analysis using the unsupervised learning techniques covered in class. You’ll extend what you’ve learned by combining PCA as a preprocessing step to clustering using data that consist of measurements of cell nuclei of human breast masses. This expands on our RNA-Seq analysis from last day.

Our data for today come form FNA breast tissue. 

## 1. Preparing the data

> **Q1.** How many observations are in this dataset?

**569 observations of 30 variables**

```{r}
wisc.df <- read.csv("https://bioboot.github.io/bimm143_S20/class-material/WisconsinCancer.csv", row.names = 1)
head(wisc.df)
```

> **Q2.** How many of the observations have a malignant diagnosis?

**212**

```{r}
sum(wisc.df$diagnosis == "M")
sum(wisc.df$diagnosis == "B")

#the best one: 
table(wisc.df$diagnosis)
```

> **Q3.** How many variables/features in the data are suffixed with _mean?

**10:**  
[1]  2  3  4  5  6  7  8  9 10 11
[1] 10

```{r}
grep("..mean", colnames(wisc.df))
length(grep("..mean", colnames(wisc.df)))
```

```{r}
ncol(wisc.df)
```

Save diagnosis for reference later

```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
```


and remove or exclude this column form any of our analysis

```{r}
wisc.data <- wisc.df[,-1]
```



Let's try clustering this data: 

Hierarchical Clustering with `hclust`

```{r}
wisc.hc <- hclust(dist(wisc.data))
plot(wisc.hc)
```



## 2. Principal Component Analysis

Let's try PCA on this data. Before doing any analysis like this we should check if our input data needs to be scalled first?  




Side-note: 

```{r}
head(mtcars)
```

```{r}
apply(mtcars, 2, mean)
```

```{r}
apply(mtcars, 2, sd)
```

Let's try PCA on this cars dataset

```{r}
pc <- prcomp(mtcars)
summary(pc)
```

```{r}
biplot(pc)
```



```{r}
pc.scale <- prcomp(mtcars, scale=TRUE)
summary(pc.scale)
biplot(pc.scale)
```

After scaling all the deviations spread the data.




# Back to our cancer data set

# Performing PCA

Do we need to scale this data set?
Yes, we do because the spread is very different. 

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
```


How well do the PCs capture the variance in the original data?

```{r}
summary(wisc.pr)
```
> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

**Proportion of Variance**
**PC1: 0.44272**
**44,27%**

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

**Number of PCs to explain at least 70% of variance: 5. For 70% of the variance, we need to consider the cumulative proportion up to the point where it exceeds 70%.**

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

**Number of PCs to explain at least 90% of variance: 24**

Our main PC score plot (a.k.a. PC plot, PC1 vs PC2, ordeiation plot)

```{r}
attributes(wisc.pr)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

**Row names are cluttering the plot and making it difficult to interpret**

```{r}
biplot(wisc.pr)
```


We need to build our own plot here:

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

**Principal component 1 is capturing a separation of malignant (red) from benign (black) samples. Principal component 2 explains more variance in the original data than principal component 3. We can see that the first plot has a cleaner cut separating the two subgroups.**

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col=diagnosis)
```


Make a nice ggplot version

```{r}
pc <- as.data.frame(wisc.pr$x)
library(ggplot2)
ggplot(pc) +
  aes(PC1, PC2, col=diagnosis) +
  geom_point()
```


## Variance explained

```{r}
v <- summary(wisc.pr)
v$importance[2,]
```

```{r}
plot(v$importance[2,], xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
barplot(v$importance[2,], ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(v$importance[2,])), las=2, axes = FALSE)
axis(2, at=v$importance[2,], labels=round(v$importance[2,],2)*100 )
```



```{r}
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC.

**-0.2608538** 

```{r}
loading_for_PC1 <- wisc.pr$rotation[, 1]
concave_points_mean <- loading_for_PC1["concave.points_mean"]
cat("Loading for concave.points_mean in PC1:", concave_points_mean, "\n")
```


## 4. Combining methods

Here we will use the results of PCA as the input to a clustering analysis. 

We start with using 3 PCs

```{r}
wisc.pr.hslust <- hclust(dist(wisc.pr$x[,1:3]), method="ward.D2")
plot(wisc.pr.hslust)
```

> Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
**height = 80**

```{r}
plot(wisc.pr.hslust)
abline(h=80, col="red")
```


> Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

("single"): The distance between two clusters is defined as the shortest distance between any two points in the clusters. It tends to produce elongated clusters.

("complete"): The distance between two clusters is defined as the maximum distance between their individual points. It tends to produce compact, spherical clusters.

("average"): The distance between two clusters is defined as the average distance between their individual points. It aims to balance between the effects of single and complete linkage.

("ward.D2"): Minimizes the variance within clusters. It is often considered robust and suitable for spherical clusters.


> Q13. How well does the newly created model with four clusters separate out the two diagnoses?

The answer:

```{r}
grps <- cutree(wisc.pr.hslust, h=80)
table(grps)
```

```{r}
table(diagnosis)
```

> Q14. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.


```{r}
table(grps, diagnosis)
```

## 5. Sensitivity/Specificity

> Q15. OPTIONAL: Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

sensitivity: TP/(TP+FN): 0.8
specificity: TN/(TN+FN): 0.9

## 6. Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q16. Which of these new patients should we prioritize for follow up based on your results?

2